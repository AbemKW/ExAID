from typing import AsyncIterator, Optional
from langchain_core.prompts import ChatPromptTemplate
from agents.base_agent import BaseAgent
from llm import llm


class LaboratoryAgent(BaseAgent):
    """Laboratory specialist agent for lab result interpretation and recommendations"""
    
    def __init__(self, agent_id: str = "LaboratoryAgent"):
        super().__init__(agent_id)
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are an expert clinical pathologist and laboratory medicine specialist "
             "in a clinical decision support system. You specialize in interpreting "
             "laboratory results and providing diagnostic insights.\n\n"
             "IMPORTANT: Use Chain of Thought reasoning. Show your thinking process step-by-step:\n"
             "1. First, identify all abnormal laboratory values\n"
             "2. Compare values to reference ranges and clinical context\n"
             "3. Look for patterns and correlations between tests\n"
             "4. Consider potential causes for each abnormality\n"
             "5. Evaluate clinical significance and urgency\n"
             "6. Formulate diagnostic insights and recommendations\n\n"
             "Always show your reasoning process explicitly. Use phrases like:\n"
             "- 'Let me analyze these lab results step by step...'\n"
             "- 'First, I notice that...'\n"
             "- 'The pattern of elevated X and Y suggests...'\n"
             "- 'This combination of findings indicates...'\n"
             "- 'Therefore, I recommend...'\n\n"
             "Your expertise includes:\n"
             "- Complete blood count (CBC) interpretation\n"
             "- Comprehensive metabolic panel (CMP) analysis\n"
             "- Cardiac biomarkers (troponin, BNP, CK-MB)\n"
             "- Inflammatory markers (CRP, ESR, procalcitonin)\n"
             "- Coagulation studies\n"
             "- Liver function tests\n"
             "- Renal function assessment\n"
             "- Endocrine and metabolic panels\n"
             "- Infectious disease markers\n\n"
             "Guidelines:\n"
             "- Interpret results in clinical context\n"
             "- Identify critical values requiring immediate attention\n"
             "- Recognize patterns suggesting specific diagnoses\n"
             "- Consider normal ranges and age-specific variations\n"
             "- Identify potential lab errors or artifacts\n"
             "- Recommend additional tests when indicated\n"
             "- Consider drug interactions affecting lab values\n\n"
             "Provide detailed laboratory interpretation with clinical significance and recommendations. "
             "Always show your step-by-step analytical process."),
            ("user", "{input}")
        ])
    
    async def act(self, input: str) -> str:
        """Interpret laboratory results and provide recommendations"""
        chain = self.prompt | self.llm
        response = await chain.ainvoke({"input": input})
        return response.content
    
    async def act_stream(self, input: str) -> AsyncIterator[str]:
        """Stream tokens as they are generated by the LLM
        
        Args:
            input: Input text for the agent
            
        Yields:
            Tokens as strings as they are generated
        """
        chain = self.prompt | self.llm
        try:
            async for chunk in chain.astream({"input": input}):
                # Handle different chunk formats from LangChain
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    if content:
                        yield content
                elif isinstance(chunk, str) and chunk:
                    yield chunk
                elif isinstance(chunk, dict) and 'content' in chunk:
                    if chunk['content']:
                        yield chunk['content']
        except ValueError as e:
            # If streaming fails, fall back to non-streaming and yield the full response
            if "No generation chunks were returned" in str(e):
                print(f"[WARNING] Streaming failed for {self.agent_id}, falling back to non-streaming mode")
                response = await self.act(input)
                # Yield the response character by character to simulate streaming
                for char in response:
                    yield char
            else:
                raise
    
    async def decide_consultation(self, findings: str, consulted_agents: set[str]) -> Optional[str]:
        """Decide if cardiology consultation is needed based on findings
        
        Args:
            findings: The laboratory agent's findings and analysis
            consulted_agents: Set of agents that have already been consulted
            
        Returns:
            "cardiology" if cardiology consultation is needed, None otherwise
        """
        # Don't request consultation if cardiology has already been consulted
        if "cardiology" in consulted_agents:
            return None
        
        consultation_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are a laboratory medicine specialist analyzing your findings to determine "
             "if a cardiology consultation is needed.\n\n"
             "Request cardiology consultation if:\n"
             "- You identified abnormal cardiac biomarkers (troponin, BNP, CK-MB, etc.)\n"
             "- Lab results suggest cardiac involvement or cardiac disease\n"
             "- You need cardiology expertise to interpret cardiac-related lab values\n"
             "- The clinical context requires cardiac assessment\n\n"
             "Do NOT request consultation if:\n"
             "- Your findings are complete and don't require cardiac expertise\n"
             "- The case has no cardiac-related concerns\n"
             "- You can provide complete interpretation without cardiology input\n\n"
             "Respond with ONLY 'cardiology' if consultation is needed, or 'none' if not needed."),
            ("user", 
             "Laboratory Findings:\n{findings}\n\n"
             "Based on these findings, do you need cardiology consultation? "
             "Respond with 'cardiology' or 'none'.")
        ])
        
        chain = consultation_prompt | self.llm
        response = await chain.ainvoke({"findings": findings})
        response_text = response.content.strip().lower()
        
        if "cardiology" in response_text:
            return "cardiology"
        return None

