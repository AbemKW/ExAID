from typing import AsyncIterator, Optional
from langchain_core.prompts import ChatPromptTemplate
from agents.base_agent import BaseAgent
from llm import llm


class CardiologyAgent(BaseAgent):
    """Cardiology specialist agent for cardiac assessment and recommendations"""
    
    def __init__(self, agent_id: str = "CardiologyAgent"):
        super().__init__(agent_id)
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are an expert cardiologist in a clinical decision support system. "
             "You specialize in cardiac assessment, interpretation of cardiac tests, "
             "and cardiovascular disease management.\n\n"
             "IMPORTANT: Use Chain of Thought reasoning. Show your thinking process step-by-step:\n"
             "1. First, review the cardiac history and risk factors\n"
             "2. Analyze cardiac symptoms and physical exam findings\n"
             "3. Interpret cardiac biomarkers and diagnostic tests\n"
             "4. Consider differential diagnoses systematically\n"
             "5. Evaluate urgency and severity\n"
             "6. Formulate evidence-based recommendations\n\n"
             "Always show your reasoning process explicitly. Use phrases like:\n"
             "- 'Let me think through this cardiac case step by step...'\n"
             "- 'First, I need to assess the cardiac risk factors...'\n"
             "- 'The elevated troponin suggests...'\n"
             "- 'Given these findings, I conclude...'\n"
             "- 'Therefore, my recommendation is...'\n\n"
             "Your expertise includes:\n"
             "- Cardiac history interpretation and risk stratification\n"
             "- ECG interpretation and cardiac imaging\n"
             "- Cardiac biomarker analysis (troponin, BNP, etc.)\n"
             "- Heart failure, arrhythmias, coronary artery disease\n"
             "- Hypertension and cardiovascular risk factors\n"
             "- Cardiac medication recommendations\n\n"
             "Guidelines:\n"
             "- Follow ACC/AHA clinical practice guidelines\n"
             "- Assess cardiovascular risk factors comprehensively\n"
             "- Consider differential diagnoses systematically\n"
             "- Provide evidence-based recommendations\n"
             "- Identify urgent cardiac conditions requiring immediate attention\n"
             "- Consider patient comorbidities and medications\n\n"
             "Provide detailed cardiac assessment with clear reasoning and recommendations. "
             "Always show your step-by-step thought process."),
            ("user", "{input}")
        ])
    
    async def act(self, input: str) -> str:
        """Perform cardiac assessment and provide recommendations"""
        chain = self.prompt | self.llm
        response = await chain.ainvoke({"input": input})
        return response.content
    
    async def act_stream(self, input: str) -> AsyncIterator[str]:
        """Stream tokens as they are generated by the LLM
        
        Args:
            input: Input text for the agent
            
        Yields:
            Tokens as strings as they are generated
        """
        chain = self.prompt | self.llm
        try:
            async for chunk in chain.astream({"input": input}):
                # Handle different chunk formats from LangChain
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    if content:
                        yield content
                elif isinstance(chunk, str) and chunk:
                    yield chunk
                elif isinstance(chunk, dict) and 'content' in chunk:
                    if chunk['content']:
                        yield chunk['content']
        except ValueError as e:
            # If streaming fails, fall back to non-streaming and yield the full response
            if "No generation chunks were returned" in str(e):
                print(f"[WARNING] Streaming failed for {self.agent_id}, falling back to non-streaming mode")
                response = await self.act(input)
                # Yield the response character by character to simulate streaming
                for char in response:
                    yield char
            else:
                raise
    
    async def decide_consultation(self, findings: str, consulted_agents: list[str]) -> Optional[str]:
        """Decide if laboratory consultation is needed based on findings
        
        Args:
            findings: The cardiology agent's findings and analysis
            consulted_agents: List of agents that have already been consulted
            
        Returns:
            "laboratory" if laboratory consultation is needed, None otherwise
        """
        # Don't request consultation if laboratory has already been consulted
        if "laboratory" in consulted_agents:
            return None
        
        consultation_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are a cardiologist analyzing your findings to determine "
             "if a laboratory consultation is needed.\n\n"
             "Request laboratory consultation if:\n"
             "- You need additional lab values to complete your cardiac assessment\n"
             "- You need interpretation of specific lab results (cardiac biomarkers, metabolic panels, etc.)\n"
             "- Lab values are mentioned but not fully analyzed in the case\n"
             "- You need laboratory expertise to interpret abnormal values\n"
             "- The clinical context requires detailed lab interpretation\n\n"
             "Do NOT request consultation if:\n"
             "- Your findings are complete and don't require lab interpretation\n"
             "- The case has no laboratory-related concerns\n"
             "- You can provide complete assessment without laboratory input\n\n"
             "Respond with ONLY 'laboratory' if consultation is needed, or 'none' if not needed."),
            ("user", 
             "Cardiology Findings:\n{findings}\n\n"
             "Based on these findings, do you need laboratory consultation? "
             "Respond with 'laboratory' or 'none'.")
        ])
        
        chain = consultation_prompt | self.llm
        response = await chain.ainvoke({"findings": findings})
        response_text = response.content.strip().lower()
        
        if response_text == "laboratory":
            return "laboratory"
        return None

