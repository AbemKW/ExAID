from typing import AsyncIterator
from langchain_core.prompts import ChatPromptTemplate
from agents.base_agent import BaseAgent
from llm import llm


class CardiologyAgent(BaseAgent):
    """Cardiology specialist agent for cardiac assessment and recommendations"""
    
    def __init__(self, agent_id: str = "CardiologyAgent"):
        super().__init__(agent_id)
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are an expert cardiologist in a clinical decision support system. "
             "You specialize in cardiac assessment, interpretation of cardiac tests, "
             "and cardiovascular disease management.\n\n"
             "IMPORTANT: Use Chain of Thought reasoning. Show your thinking process step-by-step:\n"
             "1. First, review the cardiac history and risk factors\n"
             "2. Analyze cardiac symptoms and physical exam findings\n"
             "3. Interpret cardiac biomarkers and diagnostic tests\n"
             "4. Consider differential diagnoses systematically\n"
             "5. Evaluate urgency and severity\n"
             "6. Formulate evidence-based recommendations\n\n"
             "Always show your reasoning process explicitly. Use phrases like:\n"
             "- 'Let me think through this cardiac case step by step...'\n"
             "- 'First, I need to assess the cardiac risk factors...'\n"
             "- 'The elevated troponin suggests...'\n"
             "- 'Given these findings, I conclude...'\n"
             "- 'Therefore, my recommendation is...'\n\n"
             "Your expertise includes:\n"
             "- Cardiac history interpretation and risk stratification\n"
             "- ECG interpretation and cardiac imaging\n"
             "- Cardiac biomarker analysis (troponin, BNP, etc.)\n"
             "- Heart failure, arrhythmias, coronary artery disease\n"
             "- Hypertension and cardiovascular risk factors\n"
             "- Cardiac medication recommendations\n\n"
             "Guidelines:\n"
             "- Follow ACC/AHA clinical practice guidelines\n"
             "- Assess cardiovascular risk factors comprehensively\n"
             "- Consider differential diagnoses systematically\n"
             "- Provide evidence-based recommendations\n"
             "- Identify urgent cardiac conditions requiring immediate attention\n"
             "- Consider patient comorbidities and medications\n\n"
             "Provide detailed cardiac assessment with clear reasoning and recommendations. "
             "Always show your step-by-step thought process."),
            ("user", "{input}")
        ])
    
    async def act(self, input: str) -> str:
        """Perform cardiac assessment and provide recommendations"""
        chain = self.prompt | self.llm
        response = await chain.ainvoke({"input": input})
        return response.content
    
    async def act_stream(self, input: str) -> AsyncIterator[str]:
        """Stream tokens as they are generated by the LLM
        
        Args:
            input: Input text for the agent
            
        Yields:
            Tokens as strings as they are generated
        """
        chain = self.prompt | self.llm
        try:
            async for chunk in chain.astream({"input": input}):
                # Handle different chunk formats from LangChain
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    if content:
                        yield content
                elif isinstance(chunk, str) and chunk:
                    yield chunk
                elif isinstance(chunk, dict) and 'content' in chunk:
                    if chunk['content']:
                        yield chunk['content']
        except ValueError as e:
            # If streaming fails, fall back to non-streaming and yield the full response
            if "No generation chunks were returned" in str(e):
                print(f"[WARNING] Streaming failed for {self.agent_id}, falling back to non-streaming mode")
                response = await self.act(input)
                # Yield the response character by character to simulate streaming
                for char in response:
                    yield char
            else:
                raise

